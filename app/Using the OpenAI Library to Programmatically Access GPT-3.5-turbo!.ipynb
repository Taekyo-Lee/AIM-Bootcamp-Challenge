{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai cohere tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9vDVpasRnO2FX6kXHqNqxMufVCcji', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two different products in the cryptocurrency and blockchain space.\\n\\nLangChain is a decentralized platform that aims to connect language service providers with clients in need of translation services. It utilizes blockchain technology to ensure secure and transparent transactions between users. LangChain also offers features such as smart contracts and tokens for facilitating payments and other transactions within the platform.\\n\\nLlamaIndex, on the other hand, is a cryptocurrency index that tracks the performance of a specific set of cryptocurrencies. It provides users with information and insights into the overall market trends and performance of various cryptocurrencies. LlamaIndex is not a platform for transactions or services like LangChain, but rather a tool for monitoring and analyzing the cryptocurrency market.\\n\\nIn summary, LangChain is a platform for language services and transactions using blockchain technology, while LlamaIndex is a cryptocurrency index for tracking market performance.', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1723423665, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=171, prompt_tokens=19, total_tokens=190))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain and LlamaIndex are two different blockchain projects with different purposes and functionalities:\n",
       "\n",
       "1. LangChain is a decentralized language translation platform that utilizes blockchain technology to provide secure and efficient translation services. It is designed to connect translators directly with clients in a peer-to-peer network, eliminating the need for intermediaries and reducing costs for both parties. LangChain aims to improve the quality and accuracy of translations by leveraging the expertise of a global network of professional translators.\n",
       "\n",
       "2. LlamaIndex, on the other hand, is a decentralized financial data aggregation platform that provides users with real-time data and analytics for various financial assets and markets. It collects data from multiple sources, analyzes it using advanced algorithms, and presents it in a user-friendly interface. LlamaIndex aims to help investors make informed decisions by providing them with accurate and up-to-date information on market trends, price movements, and other relevant data.\n",
       "\n",
       "In summary, LangChain focuses on language translation services, while LlamaIndex focuses on financial data aggregation and analysis. Both projects leverage blockchain technology to enhance their respective functionalities and provide value to their users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have a preference because I don't eat or drink. But if you could please hurry up and decide, I'm really hungry and this conversation about ice isn't helping!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm all about that crushed ice! It just feels extra refreshing, don't you think?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9vDYoDQiSwwsvDvGJz8PZBRSE8kzt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm all about that crushed ice! It just feels extra refreshing, don't you think?\", role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1723423850, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=19, prompt_tokens=30, total_tokens=49))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I was surprised by how stimple it was to harvest the juicy falbeans from the garden."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "We need to use the stimple falbean to secure the bolts on the machinery."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It does matter which travel option Billy selects. If he flies and takes a bus, it will take a total of 5 hours which means he will arrive after 6PM EDT. If he takes the teleporter and then a bus, it will only take a total of 1 hour which means he will arrive before 7PM EDT. Therefore, Billy should choose the teleporter option to ensure he arrives home before 7PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, it does matter which travel option Billy selects in order to ensure he gets home before 7PM EDT. \n",
       "\n",
       "If Billy chooses to fly, it will take him 3 hours to get to his destination. Since it is currently 1PM local time, he will reach his destination at 4PM. Then, he would need to take a 2-hour bus ride, arriving home at 6PM local time. However, since 6PM local time is 9PM EDT, Billy would not make it home before 7PM EDT.\n",
       "\n",
       "On the other hand, if Billy chooses to use the teleporter, it will take him no time at all to get to his destination. He can then take a 1-hour bus ride and arrive home at 2PM local time. Since 2PM local time is 5PM EDT, Billy would successfully make it home before 7PM EDT.\n",
       "\n",
       "Therefore, Billy should select the travel option of using the teleporter followed by a bus ride in order to ensure he gets home before 7PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
